# Production Docker Compose Configuration
# docker-compose.prod.yml
services:
  # Reverse Proxy - Nginx with SSL
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_cache:/var/cache/nginx
      - nginx_logs:/var/log/nginx
    depends_on:
      - web
      - server
    restart: unless-stopped
    networks:
      - frontend
      - backend

  # Web Application (Frontend)
  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
      target: runner
    expose:
      - "80"
    environment:
      - NODE_ENV=production
    restart: unless-stopped
    networks:
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Server Application (Backend)
  server:
    build:
      context: .
      dockerfile: apps/server/Dockerfile
      target: runner
    expose:
      - "3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
      - OLLAMA_URL=http://ollama:11434
      - SEQ_URL=http://seq:5341
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - GOOGLE_CX=${GOOGLE_CX}
      - SESSION_SECRET=${SESSION_SECRET}
    depends_on:
      ollama:
        condition: service_healthy
      seq:
        condition: service_started
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama AI Service - Production
  ollama:
    image: ollama/ollama:latest
    expose:
      - "11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_MAX_LOADED_MODELS=2
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    # GPU support (uncomment if NVIDIA GPU available)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Seq Logging Service - Production
  seq:
    image: datalust/seq:latest
    expose:
      - "80"
    environment:
      - ACCEPT_EULA=Y
      - SEQ_FIRSTRUN_ADMINUSERNAME=${SEQ_ADMIN_USERNAME:-admin}
      - SEQ_FIRSTRUN_ADMINPASSWORDHASH=${SEQ_ADMIN_PASSWORDHASH:-}
      - SEQ_CACHE_RESPONSEMEMORY_MIN=100MB
      - SEQ_CACHE_RESPONSEMEMORY_MAX=500MB
    restart: unless-stopped
    volumes:
      - seq_data:/data
      - seq_logs:/var/log/seq
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/api/info/diagnostics"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Langfuse Observability Service - Production
  langfuse:
    image: langfuse/langfuse:2.95
    expose:
      - "3000"
    environment:
      - DATABASE_URL=postgres://langfuse:${LANGFUSE_DB_PASSWORD}@db:5432/langfuse
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET}
      - NEXTAUTH_URL=${NEXTAUTH_URL}
      - SALT=${SALT}
      - REDIS_URL=${REDIS_URL:-redis://redis:6379}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis for Langfuse - Production
  redis:
    image: redis:7-alpine
    expose:
      - "6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PostgreSQL Database for Langfuse - Production
  db:
    image: postgres:15
    expose:
      - "5432"
    environment:
      - POSTGRES_USER=langfuse
      - POSTGRES_PASSWORD=${LANGFUSE_DB_PASSWORD}
      - POSTGRES_DB=langfuse
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d:ro
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U langfuse"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Production PostgreSQL optimizations
    command: >
      postgres
      -c shared_preload_libraries=pg_stat_statements
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true

volumes:
  ollama_data:
    driver: local
  seq_data:
    driver: local
  seq_logs:
    driver: local
  postgres_data:
    driver: local
  redis_data:
    driver: local
  nginx_cache:
    driver: local
  nginx_logs:
    driver: local