# üöÄ QUICK START GUIDE - CarGPT with Ollama

## Setup in 5 minutes (Recommended: Docker)

### üî• Method 1: Docker Development (Recommended)

Docker provides the most reliable setup with built-in logging and monitoring.

#### Prerequisites
- **Docker Desktop** installed ([Download](https://www.docker.com/products/docker-desktop))

#### Quick Setup
```bash
# 1. Clone and setup
git clone https://github.com/lucapalomba/CarGPT.git
cd CarGPT

# 2. Create environment file
cp .env.example .env
# Edit .env and add your GOOGLE_API_KEY and GOOGLE_CX

# 3. Start complete stack (one command!)
npm run docker:dev

# 4. Pull AI model (in another terminal)
docker-compose exec ollama ollama pull mistral
```

#### Access the Application
- **Frontend**: http://localhost:5174
- **Backend API**: http://localhost:3001
- **API Documentation**: http://localhost:3001/api-docs
- **Logging (Seq)**: http://localhost:5341
- **Observability**: http://localhost:3000 (Langfuse)

---

### üíª Method 2: Local Development

If you prefer running directly on your machine.

#### Prerequisites
- **Node.js 18+** ([Download](https://nodejs.org))
- **Ollama** ([Download](https://ollama.ai))

#### Quick Setup
```bash
# 1. Install Ollama
# Windows: https://ollama.ai/download
# macOS: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# 2. Download AI model
ollama pull mistral

# 3. Start Ollama (keep running)
ollama serve

# 4. Setup CarGPT
git clone https://github.com/lucapalomba/CarGPT.git
cd CarGPT
npm install
cp apps/server/.env.example apps/server/.env
# Edit apps/server/.env and add your GOOGLE_API_KEY and GOOGLE_CX

# 5. Start applications
npm run dev
```

#### Access the Application
- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:3000
- **API Documentation**: http://localhost:3000/api-docs

---

## ‚úÖ Verify it works

1. Fill the form with:
   - Requirements: `Family car with 2 kids, large trunk, safe and spacious`
   - Click "Find my perfect cars"

2. You should get a response in ~10-30 seconds

---

## üêõ Common Issues

### Docker Issues
```bash
# Check containers are running
docker-compose ps

# View logs
npm run docker:logs

# Restart everything
npm run docker:down && npm run docker:dev
```

### Local Development Issues
```bash
# Cannot connect to Ollama?
ollama serve

# Model not found?
ollama pull mistral

# Node.js server won't start?
cd CarGPT
rm -rf node_modules && npm install
```

### Performance Issues
- **Docker**: Recommended for consistent performance
- **NVIDIA GPU**: Automatically used if available
- **Slower responses**: Normal, AI processing takes time
- **Try lighter model**: `docker-compose exec ollama ollama pull phi3`

---

## üí° Why Docker is Recommended

| Feature | Docker | Local |
|---------|---------|-------|
| **Setup Complexity** | ‚úÖ One command | ‚ùå Multiple steps |
| **Environment Consistency** | ‚úÖ Guaranteed | ‚ö†Ô∏è Variable |
| **Built-in Logging** | ‚úÖ Seq + Langfuse | ‚ùå Manual setup |
| **Port Conflicts** | ‚úÖ Handled | ‚ö†Ô∏è Common |
| **GPU Support** | ‚úÖ Auto-detected | ‚ö†Ô∏è Manual |
| **Hot Reload** | ‚úÖ Yes | ‚úÖ Yes |
| **Resource Usage** | ‚ö†Ô∏è Higher | ‚úÖ Lower |

---

## ‚úÖ Verify it works

1. Fill the form with:
   - Requirements: `Family car with 2 kids, large trunk, safe and spacious`
   - Click "Find my perfect cars"

2. You should get a response in ~10-30 seconds (depends on your CPU)

---

## üêõ Common Issues

### "Cannot connect to Ollama"
```bash
# Verify Ollama is running
ollama serve
```

### "Model ministral not found"
```bash
# Download the model
ollama pull ministral

# Verify it's installed
ollama list
```

### Node.js server won't start
```bash
# Check you're in the right folder
cd CarGPT

# Reinstall dependencies
rm -rf node_modules
npm install
```

### Responses are too slow
- **Ministral** is already optimized for speed
- If you have an NVIDIA GPU, it will be used automatically
- Try a lighter model: `ollama pull phi3` and change `.env`

---

## üéØ Next Steps

### For Docker Users
```bash
# Try different AI models
docker-compose exec ollama ollama pull llama3.2

# View application logs
npm run docker:logs

# Access monitoring dashboards
# Seq: http://localhost:5341
# Langfuse: http://localhost:3000
```

### For Local Users
```bash
# Try different models
ollama pull llama3.2

# Explore more models
# Visit: https://ollama.ai/library
```

---

## üí° Tips

### Performance
- **Docker**: More consistent performance across machines
- **Mistral**: Fast and accurate - great for this project
- **GPU Acceleration**: Automatic if NVIDIA GPU detected
- **First query slower**: Model needs to load into memory

### Usage
- **Works offline**: Once AI model is downloaded
- **Zero cost**: Use as much as you want without limits
- **Privacy**: All AI processing stays on your machine

---

## üìö More Documentation

**Have questions?** Check the comprehensive documentation:

- [üè† Main README](README.md) - Complete project overview
- [üê≥ Docker Guide](DOCKER.md) - Detailed Docker setup
- [üåê Networking Help](docs/NETWORKING.md) - Troubleshooting guide
- [‚öôÔ∏è Configuration](docs/CONFIGURATION.md) - All environment variables
- [üèóÔ∏è Architecture](ARCHITECTURE.md) - System design overview
- [üìñ API Docs](docs/API.md) - Complete API reference
